{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Style_Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_qXN5p3GBOx",
        "colab_type": "text"
      },
      "source": [
        "# Neural Style Transfer\n",
        "\n",
        "Implementation of https://arxiv.org/abs/1705.06830 with an improved UNET architecture for style transferral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-tdamEkqlQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Needed due to a performance regression on InceptionNet with newer scipy versions.\n",
        "!pip install --upgrade scipy==1.3.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgIEPbgnkQRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pickle\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from fastai.basic_train import Learner, Callback\n",
        "from fastai.basic_data import DataBunch\n",
        "from fastai.vision.data import ImageList\n",
        "from fastai.vision.models import resnet50, resnet18, unet\n",
        "from fastai.vision.transform import get_transforms\n",
        "from fastai.metrics import accuracy\n",
        "from fastai.layers import PixelShuffle_ICNR\n",
        "from typing import List\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "import torchvision.transforms as tfms\n",
        "from torchvision import models\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "import math\n",
        "\n",
        "from inspect import signature\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYp7WTwCpLmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE='cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiK4O4lg_nio",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftMjaqhq_o3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://pytorch.org/hub/pytorch_vision_vgg/\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43lx3w6FUij3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnkNZHzCKc1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_list(l, frac_first: float, seed: int = None):\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    copied = deepcopy(l)\n",
        "    random.shuffle(copied)\n",
        "\n",
        "    num_first = int(frac_first * len(copied))\n",
        "    first = copied[:num_first]\n",
        "    second = copied[num_first:]\n",
        "\n",
        "    return first, second"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdQVxKYVHha4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnlabelledImageDS(Dataset):\n",
        "    def __init__(self, fnames: List[str]):\n",
        "        self.fnames = fnames\n",
        "\n",
        "        self.transforms = tfms.Compose([\n",
        "            tfms.Resize((image_size, image_size)), \n",
        "            tfms.ToTensor(),\n",
        "            tfms.Normalize(mean, std)\n",
        "        ])\n",
        "    \n",
        "    def __len__(self): return len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fname = self.fnames[index]\n",
        "        cat_name = os.path.dirname(fname).split(\"/\")[-1]\n",
        "\n",
        "        img = Image.open(fname).convert('RGB')\n",
        "\n",
        "        transformed = self.transforms(img)\n",
        "\n",
        "        return transformed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es9NwwP4_kQr",
        "colab_type": "text"
      },
      "source": [
        "## Content Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igaAHhfUkfCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Courtesy of https://forums.fast.ai/t/how-does-one-download-imagenet/40660/9\n",
        "!wget http://files.fast.ai/data/imagenet-sample-train.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJkAuTJPlem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!tar -zxvf imagenet-sample-train.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnPyAHwVEdoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_content = glob.glob(\"train/*/*\")\n",
        "len(all_content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Go9NqgyQYoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_content, valid_content = split_list(all_content, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPW2yOqsMXEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_train_ds = UnlabelledImageDS(train_content)\n",
        "content_valid_ds = UnlabelledImageDS(valid_content)\n",
        "\n",
        "# content_train_ds = UnlabelledImageDS(train_content[:1])\n",
        "# content_valid_ds = content_train_ds\n",
        "\n",
        "len(content_train_ds), len(content_valid_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494O2irT5rvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_train_ds[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actnqILxNz4Q",
        "colab_type": "text"
      },
      "source": [
        "## Style Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfU75BNzN2Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload Kaggle token first.\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqsZ6G1HMjRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download -d thedownhill/art-images-drawings-painting-sculpture-engraving"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELQFUdOUOZ8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!unzip art-images-drawings-painting-sculpture-engraving.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvc05mVnRvds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_style_unfiltered = glob.glob(\"musemart/dataset_updated/training_set/painting/*\")\n",
        "all_style = []\n",
        "\n",
        "for img in all_style_unfiltered:\n",
        "    try:\n",
        "        x = Image.open(img)\n",
        "        all_style.append(img)\n",
        "    except UnidentifiedImageError:\n",
        "        pass\n",
        "\n",
        "len(all_style)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao2RTxtWPsEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_style, valid_style = split_list(all_style, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rxWNLAPB6rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_train_ds = UnlabelledImageDS(train_style)\n",
        "style_valid_ds = UnlabelledImageDS(valid_style)\n",
        "\n",
        "# SHORT\n",
        "# style_train_ds = UnlabelledImageDS(train_style[:1])\n",
        "# style_valid_ds = style_train_ds\n",
        "\n",
        "len(style_train_ds), len(style_valid_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62nmowVo542v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_train_ds[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8lZN9bUB5CW",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEPYHKYNyBWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleTransferDS(Dataset):\n",
        "  def __init__(self, content_ds, style_ds):\n",
        "    self.content = content_ds\n",
        "    self.style = style_ds\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    content = self.content[index]\n",
        "    style = self.style[random.randint(0, len(self.style) - 1)]\n",
        "\n",
        "    # hack so there's a 'inp' and 'label' for fastai to split\n",
        "    return (content, style), (content, style)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.content)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yws3iSWR0_Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = StyleTransferDS(content_train_ds, style_train_ds)\n",
        "valid_ds = StyleTransferDS(content_valid_ds, style_valid_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7417fHji_1hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjtfmA1UYmjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch.create(train_ds, valid_ds, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_cC_NQW3l1e",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVWgQnWDH62D",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_djfm6JEc8CI",
        "colab_type": "text"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMuuoTv4vd8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Print(nn.Module):\n",
        "    def __init__(self, name, mod):\n",
        "        super().__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.mod = mod\n",
        "    \n",
        "    def forward(self, x):\n",
        "        print(\"STARTING: \", self.name)\n",
        "        return self.mod(x)\n",
        "\n",
        "\n",
        "class SameConv(nn.Module):\n",
        "    \"\"\"Conv with same padding\"\"\"\n",
        "    def __init__(self, cin: int, cout: int, kernel_size: int, stride=1):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.conv = nn.Conv2d(cin, cout, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Assumes has NCHW format\"\"\"\n",
        "        height = x.shape[2]\n",
        "        width = x.shape[3]\n",
        "        stride = self.stride\n",
        "\n",
        "        kernel_overlap = self.kernel_size // 2\n",
        "\n",
        "        pad_h = int(math.ceil(height / stride) * stride) - height + 2*kernel_overlap\n",
        "        pad_w = int(math.ceil(width / stride) * stride) - width + 2*kernel_overlap\n",
        "\n",
        "        pad_top = pad_h // 2\n",
        "        pad_bot = pad_h - pad_top\n",
        "\n",
        "        pad_left = pad_w // 2\n",
        "        pad_right = pad_w - pad_left\n",
        "\n",
        "        padded = F.pad(x, [pad_top, pad_bot, pad_left, pad_right], mode='reflect')\n",
        "\n",
        "        return self.conv(padded)\n",
        "\n",
        "\n",
        "def conv3(cin, cout, stride=1):\n",
        "  return SameConv(cin, cout, 3, stride)\n",
        "\n",
        "\n",
        "class StyleBatchNorm(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "    def forward(self, inp, scale, shift):\n",
        "        normed =  self.norm(inp)\n",
        "        return normed * scale + shift\n",
        "\n",
        "\n",
        "class StyleConv(nn.Module):\n",
        "    def __init__(self, cin, cout, kernel_size, dim_scale, dim_shift, \n",
        "                 stride=1, act=nn.ReLU):\n",
        "        super().__init__()\n",
        "\n",
        "        self.shift_dense = nn.Linear(dim_shift, cout, bias=False)\n",
        "        self.scale_dense = nn.Linear(dim_scale, cout, bias=False)\n",
        "\n",
        "        self.conv = SameConv(cin, cout, kernel_size, stride)\n",
        "        self.relu = act()\n",
        "        self.norm = StyleBatchNorm(cout)\n",
        "\n",
        "    def forward(self, x, scale, shift):\n",
        "\n",
        "        scale = self.scale_dense(scale).view([scale.shape[0], -1, 1, 1])\n",
        "        shift = self.shift_dense(shift).view([shift.shape[0], -1, 1, 1])\n",
        "\n",
        "        conv = self.relu(self.conv(x))\n",
        "        return self.norm(conv, scale, shift)\n",
        "\n",
        "\n",
        "def sconv3(cin, cout, dim_scale, dim_shift, stride=1, act=nn.ReLU):\n",
        "    return StyleConv(cin, cout, 3, dim_scale, dim_shift, stride=stride, act=act)\n",
        "\n",
        "\n",
        "class StyleResidualBlock(nn.Module):\n",
        "    def __init__(self, cin: int, dim_scale: int, dim_shift: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = sconv3(cin, cin, dim_scale, dim_shift)\n",
        "        self.conv2 = sconv3(cin, cin, dim_scale, dim_shift, act=Identity)\n",
        "    \n",
        "    def forward(self, inp, scale, shift):\n",
        "        out = self.conv2(self.conv1(inp, scale, shift), scale, shift)\n",
        "        return out + inp\n",
        "\n",
        "\n",
        "class PixelShuffleWrapper(nn.Module):\n",
        "    def __init__(self, ni, nf, scale):\n",
        "        super().__init__()\n",
        "        self.mod = PixelShuffle_ICNR(ni, nf, scale)\n",
        "    def forward(self, inp, ignore_a, ignore_b):\n",
        "        return self.mod(inp)\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "  def __init__(self, left: nn.Module, left_cin: int, right_cin: int, \n",
        "               scale_up: int, dim_scale: int, dim_shift: int):\n",
        "    super().__init__()\n",
        "    \n",
        "    if scale_up != 1:\n",
        "        self.upsample = PixelShuffleWrapper(right_cin, left_cin, scale_up)\n",
        "    else:\n",
        "        self.upsample = sconv3(right_cin, left_cin, dim_scale, dim_shift)\n",
        "    \n",
        "    self.conv1 = sconv3(left_cin * 2, left_cin, dim_scale, dim_shift)\n",
        "    \n",
        "    \n",
        "    def hook_fn(m,i,o):\n",
        "        self.stored_left = o\n",
        "\n",
        "    self.hook = left.register_forward_hook(hook_fn)\n",
        "    self.stored_left = None\n",
        "  \n",
        "\n",
        "  def forward(self, right, scale, shift):\n",
        "    left = self.stored_left\n",
        "\n",
        "    upsampled = self.upsample(right, scale, shift)\n",
        "    upsampled_size = upsampled.shape[2]\n",
        "    left_size = left.shape[2]\n",
        "\n",
        "    delta = left_size - upsampled_size\n",
        "    if delta != 0:\n",
        "        upsampled = F.interpolate(upsampled, size=left.shape[2:])\n",
        "\n",
        "    cat = torch.cat([left, upsampled], dim=1)\n",
        "\n",
        "    x = self.conv1(cat, scale, shift)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class Style_UNET(nn.Module):\n",
        "    def __init__(self, encoder:nn.Module, children, dim_scale: int, dim_shift: int, test_img_shape=(1, 3, 200, 200)):\n",
        "        super().__init__()\n",
        "\n",
        "        ident = Identity()\n",
        "        class Enc(nn.Module):\n",
        "            def __init__(self, ident, enc):\n",
        "                super().__init__()\n",
        "                self.ident = ident\n",
        "                self.enc = enc\n",
        "            def forward(self, img, scale, shift):\n",
        "                ident_out = self.ident(img)\n",
        "                return self.enc(ident_out, scale, shift)\n",
        "\n",
        "        self.encoder = Enc(ident, encoder).to(DEVICE)\n",
        "\n",
        "        all_pairs = [(ident, test_img_shape)]\n",
        "        hooks = []\n",
        "        for child in children:\n",
        "            hook = child.register_forward_hook(lambda m,i,o: all_pairs.append((m, o.shape)))\n",
        "            hooks.append(hook)\n",
        "\n",
        "        dummy_inp = torch.ones(test_img_shape).to(DEVICE)\n",
        "        dummy_scale = torch.ones(test_img_shape[0], dim_scale).to(DEVICE)\n",
        "        dummy_shift = torch.ones(test_img_shape[0], dim_shift).to(DEVICE)\n",
        "        encoder(dummy_inp, dummy_scale, dummy_shift)\n",
        "\n",
        "        for h in hooks:\n",
        "            h.remove()\n",
        "\n",
        "        assert len(all_pairs) == len(children) + 1\n",
        "\n",
        "        kept_pairs = []\n",
        "        prev = all_pairs[0]\n",
        "\n",
        "        for pair in all_pairs[1:]:\n",
        "            if pair[1][2] != prev[1][2]:\n",
        "                kept_pairs.append(prev)\n",
        "                prev = pair\n",
        "\n",
        "        kept_pairs.append(prev)\n",
        "\n",
        "        change_modules = [p[0] for p in kept_pairs]\n",
        "        change_shapes = [p[1] for p in kept_pairs]\n",
        "\n",
        "        bottom_shape = change_shapes[-1]\n",
        "\n",
        "        connecting_modules = change_modules[:-1]\n",
        "        connecting_shapes = change_shapes[:-1]\n",
        "\n",
        "        strides = []\n",
        "        slims = []\n",
        "        prev_shape = change_shapes[0]\n",
        "        for shape in change_shapes[1:]:\n",
        "            stride = prev_shape[2] // shape[2]\n",
        "            strides.append(stride)\n",
        "\n",
        "            slim = prev_shape[2] % shape[2]\n",
        "            slims.append(slim)\n",
        "            prev_shape = shape\n",
        "\n",
        "        upsamples = []\n",
        "\n",
        "        assert len(connecting_modules) == len(connecting_shapes) == len(strides) == len(slims)\n",
        "\n",
        "\n",
        "        right_cin = bottom_shape[1]\n",
        "        for down_module, down_shape, scale in zip(reversed(connecting_modules), \n",
        "                                                       reversed(connecting_shapes), \n",
        "                                                       reversed(strides)):\n",
        "            \n",
        "            upsamples.append(UpsampleBlock(down_module, down_shape[1], right_cin, scale, dim_scale, dim_shift).to(DEVICE))\n",
        "            right_cin = down_shape[1]\n",
        "\n",
        "        self.upsamples = nn.ModuleList(upsamples)\n",
        "\n",
        "    def forward(self, x, scale, shift):\n",
        "        encoded = self.encoder(x, scale, shift)\n",
        "        \n",
        "        decoded = encoded\n",
        "        for up in self.upsamples:\n",
        "            decoded = up(decoded, scale, shift)\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz7Q1-36dRyE",
        "colab_type": "text"
      },
      "source": [
        "### Style Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpnQHAX44G4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GrabNamedModule(nn.Module):\n",
        "    def __init__(self, mod: nn.Module, name: str):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mod = mod\n",
        "\n",
        "        mod_dict = dict(mod.named_children())\n",
        "        output_mod = mod_dict[name]\n",
        "\n",
        "        self.stored = None\n",
        "\n",
        "        def hook(m,i,o):\n",
        "            self.stored = o\n",
        "\n",
        "        output_mod.register_forward_hook(hook)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        _ = self.mod(*args, **kwargs)\n",
        "        return self.stored\n",
        "\n",
        "\n",
        "class StyleNet(nn.Module):\n",
        "    def __init__(self, dim_scale: int, dim_shift: int, bottleneck_size: int = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inception = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
        "        output_name = \"Mixed_6e\"\n",
        "\n",
        "        inception_hooked = GrabNamedModule(self.inception, output_name)\n",
        "\n",
        "        self.dim_scale = dim_scale\n",
        "        self.dim_shift = dim_shift\n",
        "        \n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            inception_hooked,\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(768, bottleneck_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(bottleneck_size, dim_scale + dim_shift)\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.model(x)\n",
        "\n",
        "        scale = out[:, :self.dim_scale]\n",
        "        shift = out[:, self.dim_scale:]\n",
        "\n",
        "        return scale, shift\n",
        "\n",
        "\n",
        "class NeuralStyleTransfer(nn.Module):\n",
        "    def __init__(self, transfer: nn.Module, style_net: nn.Module):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.transfer = transfer\n",
        "        self.style_net = style_net\n",
        "    \n",
        "    def forward(self, content, style):\n",
        "        scale, shift = self.style_net(style)\n",
        "        return self.transfer(content, scale, shift)\n",
        "\n",
        "\n",
        "class StyleSequential(nn.Module):\n",
        "    def __init__(self, *modules):\n",
        "        super().__init__()\n",
        "        self.mods = nn.ModuleList(modules)\n",
        "    \n",
        "    def forward(self, x, scale, shift):\n",
        "        out = x\n",
        "        \n",
        "        for mod in self.mods:\n",
        "            params = signature(mod.forward).parameters\n",
        "            if len(params) == 1:\n",
        "                out = mod(out)\n",
        "            elif len(params) == 3:\n",
        "                out = mod(out, scale, shift)\n",
        "            else:\n",
        "                raise ValueError(f\"Expected only 2 or 4 params: this has {params}\")\n",
        "            \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxvqDVe46sIy",
        "colab_type": "text"
      },
      "source": [
        "## Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbjeaZC4T5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_scale = 2758 // 2\n",
        "dim_shift = 2758 // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS5yDgGJ6Flo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_net = StyleNet(dim_scale, dim_shift)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43odf4DOy7P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = StyleSequential(\n",
        "    StyleConv(3, 32, 9, dim_scale, dim_shift, stride=1),\n",
        "    StyleConv(32, 64, 3, dim_scale, dim_shift, stride=2),\n",
        "    StyleConv(64, 128, 3, dim_scale, dim_shift, stride=2),\n",
        "    StyleResidualBlock(128, dim_scale, dim_shift),\n",
        "    StyleResidualBlock(128, dim_scale, dim_shift),\n",
        "    StyleResidualBlock(128, dim_scale, dim_shift),\n",
        "    StyleResidualBlock(128, dim_scale, dim_shift),\n",
        "    StyleResidualBlock(128, dim_scale, dim_shift)).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvYP2fVKeuWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "children = encoder.mods"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5e-yoTN6LoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unet = Style_UNET(encoder, children, dim_scale, dim_shift).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1tim4PpiJ_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample passthrough.\n",
        "img = torch.ones(2, 3, 50, 50).to(DEVICE)\n",
        "sc = torch.ones(2, dim_scale).to(DEVICE)\n",
        "sh = torch.ones(2, dim_shift).to(DEVICE)\n",
        "unet(img, sc, sh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaxp--2kkCfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NeuralStyleTransfer(unet, style_net).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMCbVqfuoT5x",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWKPtLRP62Mk",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqjgGIWFF4Ip",
        "colab_type": "text"
      },
      "source": [
        "## VGG Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMkbTSPuF6Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://github.com/pytorch/examples/blob/b9f3b2ebb9464959bdbf0c3ac77124a704954828/fast_neural_style/neural_style/vgg.py#L7\n",
        "class Vgg16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jNWG80NIkiA",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbipDeDVs6pM",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tupDmW9Q5BMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StoredHooks:\n",
        "    def __init__(self, modules):\n",
        "        self.hooks = []\n",
        "        self.stored = []\n",
        "\n",
        "        for mod in modules:\n",
        "            self.hooks.append(mod.register_forward_hook(lambda m,i,o: self.stored.append(o)))\n",
        "    \n",
        "    def reset(self):\n",
        "        self.stored = []\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        x_gram = torch.einsum(\"bijc,bijd->bcd\", x, x) / (x.shape[2] * x.shape[3])\n",
        "        y_gram = torch.einsum(\"bijc,bijd->bcd\", y, y) / (y.shape[2] * y.shape[3])\n",
        "\n",
        "        return self.mse(x_gram, y_gram)\n",
        "        \n",
        "\n",
        "class NSTLoss(nn.Module):\n",
        "    def __init__(self, style_mult: float):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.style_mult = style_mult\n",
        "        self.vgg = Vgg16(requires_grad=False).to(DEVICE)\n",
        "        self.style_loss = StyleLoss()\n",
        "        self.content_loss = nn.MSELoss()\n",
        "    \n",
        "    def forward(self, transferred, content, style):\n",
        "        \n",
        "        content_out = self.vgg(content)\n",
        "        style_out = self.vgg(style)\n",
        "        transferred_out = self.vgg(transferred)\n",
        "\n",
        "        style_loss = 0\n",
        "        for og, tr in zip(style_out, transferred_out):\n",
        "            style_loss = style_loss + self.style_loss(og, tr)\n",
        "\n",
        "        style_loss = style_loss / len(style_out)\n",
        "        \n",
        "        content_loss = self.content_loss(content_out.relu2_2, transferred_out.relu2_2)\n",
        "        \n",
        "        print(f\"Style loss: {style_loss}, Content loss: {content_loss}\")\n",
        "\n",
        "        comb = content_loss + self.style_mult * style_loss\n",
        "        print(\"combined:\", comb)\n",
        "        return comb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDbrub0ds9NR",
        "colab_type": "text"
      },
      "source": [
        "## Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ST9WgCq67Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_mult = 1/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ObWAAPA64CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = NSTLoss(style_mult)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtPHROvnYcVz",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AaXz0CI7CuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner = Learner(data, model, loss_func=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TukoSsjgqmtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNTf6mMSwFQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3cIVYc_Z3Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvT8zt9bovTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.recorder.plot(skip_end=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-LFLxTqg6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The paper specified 4M parameter updates - this is a small fraction of that.\n",
        "learner.fit(10, 1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681Ozs-kKEAw",
        "colab_type": "text"
      },
      "source": [
        "# Checking Out Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiFmH0krI81e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cont_i = content_train_ds[0].unsqueeze(0).to(DEVICE)\n",
        "style_i = style_train_ds[0].unsqueeze(0).to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "out = model(cont_i, style_i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_whY6G6n23_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_output_image(out):\n",
        "    out_scaled = out.squeeze(0) * torch.tensor(std).view(3, 1, 1).to(DEVICE) + torch.tensor(mean).view(3, 1, 1).to(DEVICE)\n",
        "    out_np = out_scaled.permute([1,2,0]).cpu().detach().numpy()\n",
        "    img_in = (out_np * 255).astype(np.uint8)\n",
        "    return Image.fromarray(img_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Ld0aokC8Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_output_image(out)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}