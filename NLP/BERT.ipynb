{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT0k4kyjREfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QEPx9mZRMRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, word_depth, num_heads, head_depth,\n",
        "               ff_activation = nn.ReLU, dropout_prob=0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.word_depth = word_depth\n",
        "    self.head_depth = head_depth\n",
        "    self.output_depth = num_heads * head_depth\n",
        "\n",
        "    self.query = nn.Linear(word_depth, self.output_depth)\n",
        "    self.key = nn.Linear(word_depth, self.output_depth)\n",
        "    self.value = nn.Linear(word_depth, self.output_depth)\n",
        "\n",
        "    self.head_combination = nn.Linear(self.output_depth, self.word_depth)\n",
        "    self.feedforwards = nn.Sequential(\n",
        "        nn.Linear(self.word_depth, self.word_depth),\n",
        "        ff_activation(),\n",
        "        nn.Linear(self.word_depth, self.word_depth)\n",
        "    )\n",
        "\n",
        "    self.norm = nn.LayerNorm(self.word_depth)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "  \n",
        "  def forward(self, inp):\n",
        "\n",
        "    # Applying the query, key, and value layers.\n",
        "    q = self.query(inp)\n",
        "    k = self.key(inp)\n",
        "    v = self.value(inp)\n",
        "\n",
        "    # Shape <- (batch, sequence, num_heads, head_depth)\n",
        "    def reshape_per_head(x):\n",
        "      initial_shape = x.shape\n",
        "      final_shape = initial_shape[:-1] + (self.num_heads, self.head_depth)\n",
        "      return x.view(final_shape)\n",
        "    \n",
        "    q = reshape_per_head(q)\n",
        "    k = reshape_per_head(k)\n",
        "    v = reshape_per_head(v)\n",
        "\n",
        "    # (batch, sequence, num_heads)\n",
        "    attention_values = q @ k.transpose(-1, -2)\n",
        "    print(f\"att val\", attention_values.shape)\n",
        "    scaled_attention_values = attention_values / math.sqrt(self.head_depth)\n",
        "\n",
        "    normalized_attention_values = torch.softmax(scaled_attention_values, dim = -1)\n",
        "\n",
        "    values = normalized_attention_values @ v\n",
        "    new_value_shape = values.shape[:-2] + (self.output_depth,) \n",
        "    values_reshaped = values.view(*new_value_shape).contiguous()\n",
        "\n",
        "    combined_heads = self.head_combination(values_reshaped)\n",
        "    final_attention = self.norm(self.dropout(combined_heads))\n",
        "\n",
        "    feedforward = self.feedforwards(final_attention)\n",
        "\n",
        "    skip_connection = self.norm(inp + self.dropout(feedforward))\n",
        "\n",
        "    final = self.norm(skip_connection)\n",
        "\n",
        "    return final\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIg96TbIwo4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_depth, max_length=512):\n",
        "    super().__init__()\n",
        "\n",
        "    self.word_embedding = nn.Embedding(vocab_size, embedding_depth)\n",
        "    self.positional_embedding = nn.Embedding(max_length, embedding_depth)\n",
        "    self.bitext_embedding = nn.Embedding(2, embedding_depth)\n",
        "\n",
        "    self.norm = nn.LayerNorm(embedding_depth)\n",
        "\n",
        "  def forward(self, inp, token_types=None):\n",
        "    \n",
        "    if token_types is None:\n",
        "      token_types = torch.zeros(inp.shape).long()\n",
        "    \n",
        "    we = self.word_embedding(inp)\n",
        "    pe = self.positional_embedding(token_types)\n",
        "    be = self.bitext_embedding(inp)\n",
        "\n",
        "    embedding = we + pe + be\n",
        "\n",
        "    return self.norm(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71QA21Zuz2om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERT(nn.Module):\n",
        "  def __init__(self, num_layers, heads_per_layer, head_depth, vocab_size, embedding_depth, max_length):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embeddings = Embeddings(vocab_size, embedding_depth, max_length=max_length)\n",
        "    layers = [EncoderLayer(embedding_depth, heads_per_layer, head_depth=head_depth) for _ in range(num_layers)]\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, inp, token_types=None):\n",
        "\n",
        "    embedded = self.embeddings(inp, token_types=token_types)\n",
        "    out = self.layers(embedded)\n",
        "\n",
        "    return out\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPPrCUI42FFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = BERT(2, 3, 4, 10, 12, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sygXe682GVH",
        "colab_type": "code",
        "outputId": "a7f4a2b4-ac8f-44f1-b07b-b1e445c6051c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "b(torch.zeros(7, 2).long())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "att val torch.Size([7, 2, 3, 3])\n",
            "att val torch.Size([7, 2, 3, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.9141, -0.0635, -0.5667, -0.2885, -0.3414,  2.0442, -1.0042,\n",
              "           1.3162, -0.4922,  0.2312,  0.6234,  0.4554],\n",
              "         [-1.9143, -0.3391,  0.0848, -0.1083,  0.6554,  1.9112, -1.2758,\n",
              "           0.3930, -1.1434,  0.4879,  0.7014,  0.5473]],\n",
              "\n",
              "        [[-1.1996, -0.9819, -1.0433,  0.3721,  0.6000,  1.6952, -0.8568,\n",
              "           0.0877, -1.3681,  0.7744,  1.1593,  0.7608],\n",
              "         [-1.6925, -0.5312, -0.1901, -0.1470,  0.4121,  1.5486, -1.2408,\n",
              "           0.6970, -1.3836,  0.6325,  1.1619,  0.7332]],\n",
              "\n",
              "        [[-1.7565,  0.0051, -0.1654,  0.4266, -0.3064,  1.8467, -1.3330,\n",
              "           0.2359, -1.2515,  0.6251,  0.9122,  0.7612],\n",
              "         [-1.1979, -0.5619, -0.4143, -0.1075,  0.5098,  2.1466, -1.4576,\n",
              "           1.0072, -1.2004,  0.4544,  0.6030,  0.2185]],\n",
              "\n",
              "        [[-0.9966,  0.3235, -0.8787,  0.4582, -0.6787,  1.7770, -1.3124,\n",
              "          -0.0932, -1.0960,  0.2931,  1.7558,  0.4479],\n",
              "         [-1.9144, -0.3934, -1.1846,  0.0758,  1.0159,  1.2933, -0.4631,\n",
              "          -0.0276, -0.7902, -0.1432,  1.2337,  1.2979]],\n",
              "\n",
              "        [[-1.7220, -0.0900, -0.7578, -0.5801,  0.7838,  1.1296, -1.1022,\n",
              "           1.1470, -0.8476, -0.1911,  1.5910,  0.6394],\n",
              "         [-0.8709, -0.6696,  0.0569,  0.1278, -1.2776,  1.9385, -1.0198,\n",
              "           0.4316, -1.3012,  0.4634,  1.0667,  1.0542]],\n",
              "\n",
              "        [[-1.0419,  0.0212, -0.9278,  0.4121, -1.1609,  1.4656, -1.1040,\n",
              "           0.2314, -1.0501,  0.6782,  1.7316,  0.7446],\n",
              "         [-0.9356,  0.3920, -1.2058,  0.3245,  0.0211,  2.3514, -0.9509,\n",
              "          -0.3206, -1.3455,  0.7554,  0.6519,  0.2622]],\n",
              "\n",
              "        [[-0.8403, -0.3830, -1.0704,  0.5376, -1.3680,  1.6596, -1.1680,\n",
              "           1.0791, -0.8119,  0.8630,  0.8642,  0.6382],\n",
              "         [-1.8340, -0.4501, -0.1388,  0.3781,  0.5212,  1.6543, -1.1068,\n",
              "           0.2655, -1.4886,  0.7876,  0.9655,  0.4461]]],\n",
              "       grad_fn=<NativeLayerNormBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDfz0lLa519C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}