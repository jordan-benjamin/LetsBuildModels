{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZNs1LchpKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HJyUKK3h6aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Cell(nn.Module):\n",
        "  def __init__(self, input_depth, hidden_depth,\n",
        "               hidden_act = nn.Tanh, output_act = nn.Sigmoid):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_depth = input_depth\n",
        "    self.hidden_depth = hidden_depth\n",
        "    self.hidden_act = hidden_act\n",
        "    self.output_act = output_act\n",
        "\n",
        "    concat_depth = input_depth + hidden_depth\n",
        "    self.inp_to_hidden = nn.Sequential(nn.Linear(concat_depth, hidden_depth), \n",
        "                                       hidden_act())\n",
        "    \n",
        "    self.hidden_to_out = nn.Sequential(nn.Linear(hidden_depth, input_depth), \n",
        "                                       output_act())\n",
        "\n",
        "    self.hidden = None\n",
        "\n",
        "  def reset(self): \n",
        "    self.hidden = None\n",
        "  \n",
        "  def forward(self, inp):\n",
        "\n",
        "    if self.hidden is None:\n",
        "      self.hidden = torch.zeros(inp.shape[0], self.hidden_depth)\n",
        "\n",
        "    \n",
        "    concat = torch.cat([inp, self.hidden], dim=1)\n",
        "\n",
        "    self.hidden = self.inp_to_hidden(concat)\n",
        "    return self.hidden_to_out(self.hidden)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMMxc3yjjoMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_depth, hidden_depth, num_layers): \n",
        "    super().__init__()\n",
        "\n",
        "    self.input_depth = input_depth\n",
        "    self.hidden_depth = hidden_depth\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.layers = [RNN_Cell(input_depth, hidden_depth) for i in range(num_layers)]\n",
        "\n",
        "  def reset(self):\n",
        "    for l in self.layers: l.reset()\n",
        "\n",
        "  # Our input sequence has shape (sequence_length, batch_size, input_depth)\n",
        "  def forward(self, inp):\n",
        "    \n",
        "    self.reset()\n",
        "\n",
        "    outs = []\n",
        "\n",
        "    for token in inp:\n",
        "      carry = token\n",
        "      for l in self.layers:\n",
        "        hidden = l(carry)\n",
        "        carry = hidden\n",
        "      \n",
        "      outs.append(carry)\n",
        "\n",
        "    return outs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmfh-i5kx2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(32, 64, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NIRklqZk5uH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "fb040905-6c06-4c9a-ad3a-e967d6a63b22"
      },
      "source": [
        "rnn(torch.randn(3, 4, 32))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n",
            "concat shape: torch.Size([4, 96])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.5104, 0.5181, 0.5090, 0.4615, 0.4776, 0.5083, 0.4941, 0.4791, 0.4758,\n",
              "          0.4989, 0.5186, 0.5183, 0.4915, 0.4548, 0.4809, 0.4851, 0.4903, 0.5486,\n",
              "          0.5178, 0.5197, 0.5123, 0.4836, 0.5497, 0.5222, 0.4808, 0.4269, 0.5072,\n",
              "          0.5192, 0.5272, 0.4310, 0.5393, 0.4524],\n",
              "         [0.5102, 0.5184, 0.5088, 0.4616, 0.4775, 0.5084, 0.4941, 0.4790, 0.4761,\n",
              "          0.4988, 0.5188, 0.5183, 0.4916, 0.4549, 0.4808, 0.4848, 0.4904, 0.5485,\n",
              "          0.5180, 0.5198, 0.5126, 0.4836, 0.5498, 0.5222, 0.4810, 0.4270, 0.5072,\n",
              "          0.5192, 0.5270, 0.4310, 0.5393, 0.4523],\n",
              "         [0.5103, 0.5180, 0.5089, 0.4613, 0.4776, 0.5084, 0.4942, 0.4789, 0.4758,\n",
              "          0.4988, 0.5186, 0.5183, 0.4916, 0.4547, 0.4808, 0.4850, 0.4902, 0.5487,\n",
              "          0.5180, 0.5200, 0.5126, 0.4835, 0.5498, 0.5223, 0.4810, 0.4269, 0.5072,\n",
              "          0.5191, 0.5273, 0.4308, 0.5393, 0.4526],\n",
              "         [0.5103, 0.5183, 0.5089, 0.4617, 0.4773, 0.5083, 0.4941, 0.4790, 0.4760,\n",
              "          0.4987, 0.5187, 0.5183, 0.4915, 0.4548, 0.4808, 0.4850, 0.4902, 0.5485,\n",
              "          0.5179, 0.5198, 0.5124, 0.4834, 0.5496, 0.5224, 0.4808, 0.4270, 0.5072,\n",
              "          0.5192, 0.5271, 0.4309, 0.5393, 0.4522]], grad_fn=<SigmoidBackward>),\n",
              " tensor([[0.4966, 0.5155, 0.5105, 0.4263, 0.4814, 0.5060, 0.4825, 0.4823, 0.4508,\n",
              "          0.5106, 0.5263, 0.5254, 0.4952, 0.4454, 0.4815, 0.4778, 0.4805, 0.5513,\n",
              "          0.5342, 0.5084, 0.5179, 0.4881, 0.5365, 0.5155, 0.4671, 0.4304, 0.5122,\n",
              "          0.5116, 0.5118, 0.4271, 0.5392, 0.4449],\n",
              "         [0.4967, 0.5156, 0.5105, 0.4260, 0.4813, 0.5059, 0.4826, 0.4822, 0.4510,\n",
              "          0.5104, 0.5263, 0.5256, 0.4951, 0.4453, 0.4813, 0.4776, 0.4804, 0.5514,\n",
              "          0.5343, 0.5084, 0.5179, 0.4882, 0.5365, 0.5154, 0.4671, 0.4302, 0.5120,\n",
              "          0.5113, 0.5118, 0.4268, 0.5394, 0.4450],\n",
              "         [0.4965, 0.5157, 0.5106, 0.4262, 0.4814, 0.5061, 0.4825, 0.4823, 0.4510,\n",
              "          0.5106, 0.5262, 0.5255, 0.4954, 0.4455, 0.4815, 0.4775, 0.4808, 0.5513,\n",
              "          0.5344, 0.5083, 0.5182, 0.4883, 0.5366, 0.5154, 0.4671, 0.4305, 0.5120,\n",
              "          0.5116, 0.5118, 0.4271, 0.5393, 0.4450],\n",
              "         [0.4966, 0.5156, 0.5107, 0.4261, 0.4815, 0.5059, 0.4826, 0.4822, 0.4509,\n",
              "          0.5107, 0.5263, 0.5257, 0.4953, 0.4453, 0.4816, 0.4777, 0.4806, 0.5513,\n",
              "          0.5343, 0.5085, 0.5180, 0.4881, 0.5365, 0.5155, 0.4670, 0.4304, 0.5121,\n",
              "          0.5115, 0.5120, 0.4271, 0.5394, 0.4450]], grad_fn=<SigmoidBackward>),\n",
              " tensor([[0.4911, 0.5198, 0.5069, 0.4239, 0.4812, 0.5071, 0.4800, 0.4885, 0.4621,\n",
              "          0.5071, 0.5283, 0.5261, 0.5064, 0.4479, 0.4779, 0.4760, 0.4813, 0.5364,\n",
              "          0.5369, 0.5094, 0.5096, 0.4864, 0.5415, 0.5147, 0.4634, 0.4288, 0.5040,\n",
              "          0.5128, 0.5144, 0.4264, 0.5382, 0.4415],\n",
              "         [0.4908, 0.5200, 0.5068, 0.4241, 0.4811, 0.5074, 0.4799, 0.4885, 0.4622,\n",
              "          0.5072, 0.5282, 0.5259, 0.5063, 0.4480, 0.4782, 0.4761, 0.4812, 0.5364,\n",
              "          0.5367, 0.5091, 0.5096, 0.4864, 0.5417, 0.5147, 0.4633, 0.4288, 0.5040,\n",
              "          0.5130, 0.5143, 0.4266, 0.5381, 0.4413],\n",
              "         [0.4913, 0.5201, 0.5068, 0.4241, 0.4813, 0.5073, 0.4800, 0.4884, 0.4619,\n",
              "          0.5073, 0.5280, 0.5258, 0.5060, 0.4478, 0.4780, 0.4762, 0.4809, 0.5363,\n",
              "          0.5366, 0.5090, 0.5091, 0.4868, 0.5415, 0.5147, 0.4633, 0.4286, 0.5042,\n",
              "          0.5132, 0.5142, 0.4264, 0.5379, 0.4414],\n",
              "         [0.4910, 0.5200, 0.5071, 0.4242, 0.4810, 0.5070, 0.4800, 0.4885, 0.4622,\n",
              "          0.5073, 0.5283, 0.5261, 0.5064, 0.4480, 0.4782, 0.4759, 0.4814, 0.5362,\n",
              "          0.5368, 0.5091, 0.5095, 0.4864, 0.5413, 0.5147, 0.4631, 0.4289, 0.5040,\n",
              "          0.5130, 0.5144, 0.4266, 0.5383, 0.4413]], grad_fn=<SigmoidBackward>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTqvKeYlOAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}